# ğŸ§  Autonomous AI Insight Engine

A mini autonomous LLM agent that processes user research queries, retrieves context from a vector store, and returns structured, source-cited, hallucination-free answers â€” with conversational memory, caching, real-time streaming, and SQL logging.

## ğŸš€ Features

- âœ… LLM-powered structured answers  
- âœ… Embedding-based retrieval (Chroma + Ollama)  
- âœ… FastAPI endpoint (`POST /query`)  
- âœ… LangGraph with memory: remembers prior messages  
- âœ… Cached answers for identical/similar queries (no re-retrieval)  
- âœ… Real-time token streaming (`/query/stream`)  
- âœ… PostgreSQL logging of every query + answer  
- âœ… Source citation and hallucination control  

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py               # FastAPI app
â”œâ”€â”€ llm_agent.py          # LangGraph agent logic, memory, generation
â”œâ”€â”€ retriever.py          # Embedding + Chroma vector DB
â”œâ”€â”€ db.py                 # SQLAlchemy + PostgreSQL logging setup
â”œâ”€â”€ data/                 # Folder with .txt/.pdf research docs
â”œâ”€â”€ requirements.txt      # Project dependencies
â”œâ”€â”€ README.md             # You're here
â””â”€â”€ example_response.json # Sample query + result
```

## ğŸ§ª Setup Instructions

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

(Ensure Ollama is installed and PostgreSQL is running)

### 2. PostgreSQL Setup

Make sure a PostgreSQL server is running locally, then create the DB:

```sql
CREATE DATABASE perceivenow_db;
```

Update your `DATABASE_URL` in `db.py` like:

```python
DATABASE_URL = "postgresql+psycopg2://postgres:your_password@localhost:5432/perceivenow_db"
```

### 3. Add documents

Put at least 10 `.txt` or `.pdf` files in the `data/` folder.

### 4. Run the app

```bash
uvicorn main:app --reload
```

Open Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)

## ğŸ“® API Usage

### `POST /query`

**Request:**
```json
{
  "query": "What are the top 3 use cases for GraphQL in enterprise SaaS?"
}
```

**Response:**
```json
{
  "answer": "- Efficient data fetching\n- Developer productivity...\n[Source: doc_01.txt]",
  "sources": ["doc_01.txt", "doc_03.pdf"]
}
```

### `POST /query/stream`

Same input as `/query` but returns tokens one-by-one using Server-Sent Events (SSE). Useful for real-time UIs.

## ğŸ§  Memory, Caching & Logging

This engine uses LangGraphâ€™s `MemorySaver` with per-thread memory (`thread_id`) so it remembers past interactions. If a user asks the same or a related question, the answer will be served from memory â€” without re-triggering embedding or retrieval.

Every query + generated answer + sources are logged into a **PostgreSQL table `query_logs`**, with timestamps for tracking and analytics.

## ğŸ§± Powered By

- [LangGraph](https://github.com/langchain-ai/langgraph)  
- [LangChain](https://github.com/langchain-ai/langchain)  
- [Ollama](https://ollama.com)  
- [ChromaDB](https://www.trychroma.com)  
- [PostgreSQL](https://www.postgresql.org/)  

## âœ… Bonus

- Token-by-token streaming  
- Memory-optimized retrieval  
- PostgreSQL history logging  
- Ready for frontend integration

## ğŸ‘¨â€ğŸ’» Author

Built with love for the Perceive Now technical challenge ğŸ’¡
